# CGT-Final
In this study, the algorithm "policy hill-climbing" (PHC) and its suggested improved version, "PHC win-or-learn-fast" (PHC-WoLF), will be compared. We evaluate these algorithms' performances using a variety of games. We begin by testing the two algorithms on simple games such as Matching-Pennies, in which two players bet on the outcome of a coin flip, and Rock-Paper-Scissors, a game with three possible actions played by two players where each action can beat another one. Next, we will move on to more complex games, starting with GridWorld, a two-player game set in a 9-square environment where two players must race to reach a target cell. This is a general-sum game which allows for both cooperative and defecting behaviour. We will observe how the two algorithms perform in this setting. Finally, we will investigate the "Soccer" game, described in the (Littman, M. L. (1994). Markov games as a framework for multi-
agent reinforcement learning. In Machine learning proceedings 1994, pages 157–163. Elsevier) paper, which is a zero-sum game in which two players compete to score in their opponent's goal. We present a detailed analysis of each game, including their implementation, reward functions, and expected results. We will compare the performance of both the PHC and PHC-WoLF algorithms for each game and discuss the differences in their performances. Our goal is to replicate the results of paper (Bowling, M. and Veloso, M. (2001). Rational and convergent learning in stochastic games. In International joint conference on artificial intelligence, volume 17, pages 1021–1026. Lawrence Erlbaum Associates Ltd) and explore the potential of further improvement based on the PHC-WoLF algorithm by developing our variation. 
